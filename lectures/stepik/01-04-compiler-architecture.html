<h2 style="text-align:center">Compiler Architecture</h2>

<p>The majority of real compilers share the same architectural features. They consist of a number of <em>passes</em>, each of which takes as input and returns as a result a certain <em>intermediate representation</em>, <em>IR</em>, (not necessarily the same) of a program being compiled. The concrete set of these passes and concrete forms of intermediate representations vary from a compiler to a compiler, but there are still some common representation which can be found throughout various implementations: <em>abstract syntax tree</em>, <em>three-address code</em>, <em>stack machine code</em>, <em>static single assignment</em> (SSA), etc. Some of these forms of representation will be utilized in our compiler, some others will be left out unused.</p>

<p>All the diversity of passes can be subdivided in a few categories.</p>

<p style="text-align:center"><img alt="" height="197" name="01-06.png" src="https://ucarecdn.com/803e9692-37c9-4744-8174-5e93b2eff061/" width="567" /></p>

<p>First, the <em>frontend</em> of a compiler collects a number of <em>analyzing</em> passes: syntax analysis, type inference/checking, name resolution, etc.</p>

<p>Then, the <em>backend</em> of a compiler contains code generation passes: instruction selection, register allocation, instruction scheduling, low-level optimizations.</p>

<p>Finally, there can be <em>middle-end</em>, which comprised of various machine-independent optimising passes such as jump optimizations, dead code elimination, loop invariant code motion, common subexpression elimination, etc.</p>

<p>Most compilers allow to peep on what&#39;s going on under the hood. For example, <tt>GCC</tt> accepts the option <tt>-time-report</tt>, which makes it output the time taken by each pass performed. An example of such an output is shown below:</p>

<pre>
<code>Execution times (seconds)
 phase setup             :   0.00 ( 0%) usr   0.00 ( 0%) sys   0.00 ( 0%) wall    1179 kB (25%) ggc
 phase parsing           :   0.03 (33%) usr   0.01 (100%) sys   0.04 (40%) wall    1197 kB (25%) ggc
 phase opt and generate  :   0.06 (67%) usr   0.00 ( 0%) sys   0.06 (60%) wall    2328 kB (49%) ggc
 callgraph optimization  :   0.01 (11%) usr   0.00 ( 0%) sys   0.00 ( 0%) wall       0 kB ( 0%) ggc
 cfg cleanup             :   0.00 ( 0%) usr   0.00 ( 0%) sys   0.01 (10%) wall       0 kB ( 0%) ggc
 trivially dead code     :   0.00 ( 0%) usr   0.00 ( 0%) sys   0.01 (10%) wall       0 kB ( 0%) ggc
 df reaching defs        :   0.01 (11%) usr   0.00 ( 0%) sys   0.00 ( 0%) wall       0 kB ( 0%) ggc
 alias analysis          :   0.01 (11%) usr   0.00 ( 0%) sys   0.00 ( 0%) wall      55 kB ( 1%) ggc
 preprocessing           :   0.00 ( 0%) usr   0.00 ( 0%) sys   0.01 (10%) wall     345 kB ( 7%) ggc
 lexical analysis        :   0.03 (33%) usr   0.01 (100%) sys   0.01 (10%) wall       0 kB ( 0%) ggc
 parser inl. func. body  :   0.00 ( 0%) usr   0.00 ( 0%) sys   0.02 (20%) wall      79 kB ( 2%) ggc
 dominator optimization  :   0.00 ( 0%) usr   0.00 ( 0%) sys   0.01 (10%) wall      45 kB ( 1%) ggc
 forward prop            :   0.01 (11%) usr   0.00 ( 0%) sys   0.00 ( 0%) wall      13 kB ( 0%) ggc
 loop init               :   0.02 (22%) usr   0.00 ( 0%) sys   0.01 (10%) wall     193 kB ( 4%) ggc
 loop fini               :   0.00 ( 0%) usr   0.00 ( 0%) sys   0.01 (10%) wall       0 kB ( 0%) ggc
 final                   :   0.00 ( 0%) usr   0.00 ( 0%) sys   0.01 (10%) wall      48 kB ( 1%) ggc
 TOTAL                 :   0.09             0.01             0.10               4714 kB</code></pre>

<p>&nbsp;</p>

<p style="text-align:center"><img alt="Toolchain" height="883" name="image.png" src="https://ucarecdn.com/78b4cba7-946f-4cd1-b75d-6a06fcd3d996/" width="855" /></p>

<h2 style="text-align:center">Beyond Compilers</h2>

<p>While compilers, indeed, transform source programs to machine code, the code they output, as a rule, cannot be directly run on hardware. The reason is that it still contains certain abstractions the hardware cannot deal with. In particular, compilers usually generate assembler code with <em>symbolic names</em>. This approach plays an important role in supporting <em>separate compilation</em> &mdash; a feature which allows to combine programs from a number of precompiled modules. The majority of application programs make use of various libraries; as a rule, the source code of these libraries is not compiled alongside with the application itself, but <em>linked</em> at post-compilation time, thus greatly reducing the time required for build. To make it possible, compilers produce not ready-to-run binary code, but so-called <em>object files</em> which, besides machine code, contain supplementary <em>metadata</em> to make linking possible. Thus, on the way to the real executable binary the output which compilers provide is transformed again a number of times.</p>

<p>In the most general case, the compiler generates a textual assembler program. Then the following tools can be used:</p>

<ol>
  <li><em>assembler</em> which reads assembler programs and outputs object files;</li>
  <li><em>archiver/librarian</em> which combines multiple object files into one archive/library file;</li>
  <li><em>linker</em> which combines multiple object and library files into one executable.</li>
</ol>

<p>These tools together with a compiler form so-called <em>toolchain</em>. When a new processor comes to market it is expected from the vendor to supply a conventional toolchain for the developers. In <span style="font-variant-caps: all-small-caps">Unix</span>-like systems the conventional toolchain consists of separate programs <b>cpp</b> (C preprocessor), <b>cc</b> (C compiler), <b>as</b> (assembler), <b>ld</b> (linker) and <b>ar</b> (archiver). <tt>GCC</tt> compiler implements only two first of these components &mdash; <b>cpp</b> and <b>cc</b>, &mdash; and invokes others to complete the compilation process depending on what options were specified by users. The top-level program <b>gcc</b> itself is just a <em>driver</em> which controls the execution of other tools of the toolchain.</p>

<h2 style="text-align:center">The &lambda;aMa Compiler</h2>

<p>Now, when we discussed a little how compilers work, let&#39;s have a look at &lambda;aMa&nbsp;compiler, which will be our main tool throughout the course, and which we will be implementing. </p>

<p>The &lambda;aMa compiler is organized much simpler than the majority of industrial-tier compilers like <tt>GCC</tt>, which we already mentioned multiple times. </p>

<p style="text-align:center"><img alt="" height="782" name="image.png" src="https://ucarecdn.com/ed355a76-7789-41d0-a571-7c5e6c303991/" width="720" /></p>

<p style="text-align:center">The Structure of &lambda;aMa Compiler</p>

<p>The source file with a&nbsp;&lambda;aMa program is parsed by a syntax analyser which converts it into an abstract syntax tree, or AST. An example of a program and its AST (actually, an <em>HTML-rendering</em> of its AST) is shown below: </p>

<p><code>printf (&quot;Hello, world!\n&quot;)</code><img alt="" height="216" name="01-08.png" src="https://ucarecdn.com/e2004152-d411-4f16-8123-890eeee3b387/" width="428" /></p>

<p>AST is one of the most important program representations, and its use in compilers and other tools is ubiquitous. </p>

<p>The next component, which is rarely implemented in real-world compilers, is a source-level interpreter. We consider this object in details later, for now it is sufficient to know that interpreter is a component which directly runs a program in some representation &mdash; in our case, in the form of AST, &mdash; according to the semantics of the language. The presence of interpreter plays an important role from both educational and technological standpoints. First, the implementation of interpreter facilitates the internalization of formal semantics description method which we use, namely &mdash; big-step operational semantics. Next, it allows to find and eliminate some errors at early stage. The implementation of interpreter is rather a simple task, and it is advantageous to be capable of running program at early stages of compiler implementation. </p>

<p>Then, there is a compiler of AST into <em>stack machine</em> code. This machine resembles a simplified model of actual hardware processor; thus, on one hand, to generate stack machine code a similar set of tasks has to be solved; on the other, these solutions are a bit simpler than for an actual hardware. For our example program the corresponding stack machine code looks like </p>

<pre>
<code>LABEL (&quot;main&quot;)
BEGIN (&quot;main&quot;, 2, 0, [], [], [])
STRING (&quot;Hello, world!\\n&quot;)
CALL (&quot;Lprintf&quot;, 1, false)
END     </code></pre>

<p>Generated stack machine code can then be run on the <em>stack machine interpreter</em>. Similarly to source-level interpreter case, the capability to run stack machine code makes it possible to develop and debug stack machine compiler in isolation.</p>

<p>Finally, the last component of the compiler is code generator for <tt>x86</tt> processor which transforms stack machine code into <tt>x86</tt> assembler program; this program is then passed to the <tt>GCC</tt> infrastructure to be finally transformed into an object module. An example of <tt>x86</tt> assembler listing for our example program is shown below: </p>

<pre>
<code>         .globl main
         .data
string_0:.string &quot;Hello, world!\n&quot;
main:
# BEGIN (&quot;main&quot;, 2, 0, [], [], []) /
# STRING (&quot;Hello, world!\\n&quot;) /
         movl  $string_0, %ebx
  pushl    %ebx
  call Bstring
  addl $4,    %esp
  movl %eax,  %ebx
# CALL (&quot;Lprintf&quot;, 1, false) /
  pushl    %ebx
  call Lprintf
  addl $4,    %esp
  movl %eax,  %ebx
# END /
  movl %ebx,  %eax
Lmain_epilogue:
         movl  %ebp,  %esp
  popl %ebp
  xorl %eax,  %eax
  ret</code></pre>

<p>Thus, from the architectural point of view, syntax analyser constitutes a frontend, while compilers for stack machine and <tt>x86</tt> &mdash; a backend.</p>
